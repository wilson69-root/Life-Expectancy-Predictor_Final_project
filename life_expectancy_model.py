# -*- coding: utf-8 -*-
"""life_expectancy_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qEZeo-QTUJjP5K9vN4LrLiMdD-cm1BRt
"""

import pandas as pd

df = pd.read_csv('data/Life Expectancy Data.csv')
df.head()

# Drop columns with too many missing values or fill them
df.drop(['Country', 'Year'], axis=1, inplace=True)
df = df.dropna()

# Convert categorical column 'Status' to numerical using one-hot encoding
df = pd.get_dummies(df, columns=['Status'], drop_first=True)

# Normalize data (optional for some models)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df.drop('Life expectancy ', axis=1))
y = df['Life expectancy ']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("R²:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

import matplotlib.pyplot as plt
import seaborn as sns

# Feature Importance
feat_imp = pd.Series(model.feature_importances_,
                     index=df.drop('Life expectancy ', axis=1).columns)
feat_imp.nlargest(10).plot(kind='barh')
plt.title('Top Predictors of Life Expectancy')
plt.show()

"""# Task
Train two additional regression models (Linear Regression and XGBoost) on the provided data, evaluate their performance using R², MAE, and RMSE, and create a model comparison table including the previously trained Random Forest model to compare the performance metrics and add relevant notes for each model.

## Train linear regression model

### Subtask:
Train a Linear Regression model on the training data (`X_train`, `y_train`).

**Reasoning**:
Train a Linear Regression model on the training data.
"""

from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

"""## Evaluate linear regression model

### Subtask:
Evaluate the performance of the Linear Regression model using R², MAE, and RMSE on the testing data (`X_test`, `y_test`).

**Reasoning**:
Evaluate the performance of the trained Linear Regression model on the test set using R², MAE, and RMSE.
"""

y_pred_linear = linear_model.predict(X_test)

r2_linear = r2_score(y_test, y_pred_linear)
mae_linear = mean_absolute_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))

print(f"Linear Regression R²: {r2_linear}")
print(f"Linear Regression MAE: {mae_linear}")
print(f"Linear Regression RMSE: {rmse_linear}")

"""## Train xgboost model

### Subtask:
Train an XGBoost Regressor model on the training data (`X_train`, `y_train`).

**Reasoning**:
Train an XGBoost Regressor model on the training data.
"""

import xgboost as xgb

xgb_model = xgb.XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)

"""## Evaluate xgboost model

### Subtask:
Evaluate the performance of the XGBoost model using R², MAE, and RMSE on the testing data (`X_test`, `y_test`).

**Reasoning**:
Make predictions using the trained XGBoost model and calculate the evaluation metrics (R², MAE, RMSE) as per the instructions.
"""

y_pred_xgb = xgb_model.predict(X_test)

r2_xgb = r2_score(y_test, y_pred_xgb)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))

print(f"XGBoost R²: {r2_xgb}")
print(f"XGBoost MAE: {mae_xgb}")
print(f"XGBoost RMSE: {rmse_xgb}")

"""## Create model comparison table

### Subtask:
Create a pandas DataFrame to store the R², MAE, and RMSE for each of the three models (Random Forest, Linear Regression, and XGBoost), along with any relevant notes.

**Reasoning**:
Create a DataFrame to compare the performance metrics of the three models.
"""

# Retrieve metrics from previous outputs and variables
r2_rf = r2_score(y_test, y_pred)
mae_rf = mean_absolute_error(y_test, y_pred)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred))

model_performance = {
    'Random Forest': [r2_rf, mae_rf, rmse_rf],
    'Linear Regression': [r2_linear, mae_linear, rmse_linear],
    'XGBoost': [r2_xgb, mae_xgb, rmse_xgb]
}

model_comparison_df = pd.DataFrame(model_performance, index=['R²', 'MAE', 'RMSE'])

notes = {
    'Random Forest': 'Baseline model',
    'Linear Regression': 'Good performance',
    'XGBoost': 'Strong performance'
}

model_comparison_df['Notes'] = model_comparison_df.columns.map(notes)

print(model_comparison_df)

"""## Display model comparison table

### Subtask:
Display the created model comparison table.

**Reasoning**:
Display the model comparison table as requested.
"""

print(model_comparison_df)

"""## Summary:

### Data Analysis Key Findings

*   The Linear Regression model achieved an R² of 0.8105, an MAE of 2.8132, and an RMSE of 3.6685 on the test set.
*   The XGBoost model achieved an R² of 0.9363, an MAE of 1.2407, and an RMSE of 2.1268 on the test set.
*   A model comparison table was created, showing that XGBoost had the best performance across all metrics (highest R², lowest MAE, and lowest RMSE) compared to Linear Regression and the previously trained Random Forest model.
*   Notes were added to the comparison table: 'Baseline model' for Random Forest, 'Good performance' for Linear Regression, and 'Strong performance' for XGBoost.

### Insights or Next Steps

*   The XGBoost model demonstrates significantly better predictive performance than Linear Regression and Random Forest for this dataset, suggesting it is the most suitable model for this regression task.
*   Further tuning of the XGBoost model's hyperparameters could potentially lead to even better performance.

# Task
Implement Explainable AI (XAI) on a trained RandomForestRegressor model using the SHAP library. Use the Life Expectancy WHO dataset and display:

A summary plot showing feature impact on predictions

A bar plot of top 10 features by SHAP value

An explanation for a single prediction

Assumptions:

The dataset is already cleaned and scaled as X_test

The model is already trained and named model

The target variable is Life expectancy

Please install SHAP if not available, and ensure output visualizations are rendered in the notebook.

## Install shap

### Subtask:
Install the SHAP library if it is not already installed.

**Reasoning**:
Install the SHAP library using pip.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install shap

"""## Calculate shap values

### Subtask:
Calculate the SHAP values for the test dataset using the trained Random Forest model.

**Reasoning**:
Calculate SHAP values for the test set to prepare for explanation and visualization.
"""

import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

"""## Generate summary plot

### Subtask:
Generate a SHAP summary plot to show the impact of each feature on the model's predictions.

**Reasoning**:
Generate a SHAP summary plot to visualize the impact of each feature on the model's predictions, using the calculated SHAP values and the test data with feature names.
"""

X_test_df = pd.DataFrame(X_test, columns=df.drop('Life expectancy ', axis=1).columns)
shap.summary_plot(shap_values, X_test_df)

"""## Generate bar plot of top features

### Subtask:
Generate a bar plot showing the mean absolute SHAP values for the top 10 features.

**Reasoning**:
Calculate the mean absolute SHAP values, select the top 10 features, and generate a horizontal bar plot to visualize them.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)
mean_abs_shap_series = pd.Series(mean_abs_shap_values, index=X_test_df.columns)
top_10_features = mean_abs_shap_series.nlargest(10)

plt.figure(figsize=(10, 6))
sns.barplot(x=top_10_features.values, y=top_10_features.index)
plt.title('Top 10 Features by Mean Absolute SHAP Value')
plt.xlabel('Mean Absolute SHAP Value')
plt.ylabel('Feature')
plt.show()

"""## Explain a single prediction

### Subtask:
Choose an instance from the test set and generate a SHAP force plot to explain that single prediction.

**Reasoning**:
Generate a SHAP force plot for a single instance to explain its prediction.
"""

instance_index = 10 # Choosing an arbitrary instance index
shap.initjs() # Initialize JavaScript for interactive plots
shap.force_plot(explainer.expected_value, shap_values[instance_index], X_test_df.iloc[instance_index])

"""## Summary:

### Data Analysis Key Findings

*   The SHAP library was successfully installed and used to analyze the trained `RandomForestRegressor` model.
*   The SHAP summary plot indicated the distribution and impact of each feature on the model's predictions.
*   The bar plot of mean absolute SHAP values identified the top 10 features with the highest average impact on the life expectancy predictions.
*   A SHAP force plot was generated to visually explain the contribution of each feature to the prediction for a specific instance in the test set.

### Insights or Next Steps

*   The analysis revealed the most influential features driving the model's life expectancy predictions, which can inform future data collection or feature engineering efforts.
*   The single prediction explanation provides a powerful tool for understanding individual predictions and building trust in the model's output. Further exploration of specific instances can highlight model behavior.

"""

import joblib

# Save the model to a file
joblib.dump(model, 'random_forest_model.pkl')

print("Random Forest model saved successfully as 'random_forest_model.pkl'")